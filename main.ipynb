{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for l in os.listdir(\"dataset/fermi_excel/Fermate\"):\n",
    "    df = pd.read_excel(f\"dataset/fermi_excel/Fermate/{l}\")\n",
    "    name = l.replace(\".xls\", \"\")\n",
    "\n",
    "    df.to_csv(f\"dataset/fermi/Fermate/{name}.csv\", index=False)\n",
    "\n",
    "df = pd.read_excel(f\"dataset/fermi_excel/FERMATE 2211 ACR.xls\")\n",
    "df.to_csv(f\"dataset/fermi/FERMATE 2211 ACR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I iterate for every machine to get the data from the 3 datasets and merge them into one dataset, removing the ones that are not useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT1 ~ Now we can check for all the dataset and see what we can do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.getDataset import getEntireDataset, forEveryMachine, cleanDataset\n",
    "\n",
    "DEBUG = True\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.options.mode.copy_on_write = False\n",
    "\n",
    "most_interesting = []\n",
    "\n",
    "def fn(machineId, year, month):\n",
    "    dataset = getEntireDataset(machineId, year, month, False)\n",
    "\n",
    "    if dataset.empty:\n",
    "        print(\"Skipping as empty\", machineId, year, month)\n",
    "        return\n",
    "\n",
    "    # if dataset[\"EnergyConsumption\"].dropna().eq(0).all():\n",
    "    #     if DEBUG:\n",
    "    #         print(\"Skipping as EnergyConsumption is all 0\")\n",
    "    #     return\n",
    "    # if dataset[\"Fermate\"].dropna().eq(0).all():\n",
    "    #     if DEBUG:\n",
    "    #         print(\"Skipping as Fermate is all 0\")\n",
    "    #     return\n",
    "\n",
    "    # EC_na = dataset[\"EnergyConsumption\"].isna().sum()\n",
    "    # EC_len = dataset[\"EnergyConsumption\"].shape[0]\n",
    "\n",
    "    # ST_na = dataset[\"Fermate\"].isna().sum()\n",
    "    # ST_len = dataset[\"Fermate\"].shape[0]\n",
    "\n",
    "    # energyRate = EC_na/ EC_len\n",
    "    # stopsRate = ST_na/ ST_len\n",
    "\n",
    "    # if energyRate > 0.5 or stopsRate > 0.5:\n",
    "    #     if DEBUG:\n",
    "    #         print(\"Skipping as too many NaN values\")\n",
    "    #         print(\"\\tEnergy Rate:\", energyRate)\n",
    "    #         print(\"\\tStops Rate:\", stopsRate)\n",
    "    #     return\n",
    "\n",
    "    dataset = cleanDataset(dataset)\n",
    "    dataset.to_csv(\n",
    "        f\"dataset/results/it1/id-{machineId}_{year}-{month}.csv\", index=False\n",
    "    )\n",
    "\n",
    "    most_interesting.append((dataset, machineId, year, month))\n",
    "\n",
    "forEveryMachine(fn)\n",
    "\n",
    "print(\"\\n\\n\\nThe most interesting are the followings:\")\n",
    "for dataset, machineId, year, month in most_interesting:\n",
    "    print(f\"- Machine {machineId} - {year}/{month}\")\n",
    "\n",
    "# for dataset, machineId, year, month in most_interesting:\n",
    "#     plot(dataset, machineId, year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT2 ~ It took 20 minutes to compute everything but now I want to look at the lifetime of the machines and see if I can find something interesting, merging the data into one file for machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.getSingleDataset.utils import getCleanDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "compete_dataset = {}\n",
    "\n",
    "for path in os.listdir(\"dataset/results/it1\"):\n",
    "    # dataset.to_csv(f\"dataset/results/it1/id-{machineId}_{year}-{month}.csv\")\n",
    "\n",
    "    splitted_filename = [p.split(\"-\") for p in path.replace(\".csv\", \"\").split(\"_\")]\n",
    "\n",
    "    machineId = int(splitted_filename[0][1])\n",
    "    year = int(splitted_filename[1][0])\n",
    "    month = int(splitted_filename[1][1])\n",
    "\n",
    "    d = getCleanDataset(f\"dataset/results/it1/id-{machineId}_{year}-{month}.csv\")\n",
    "\n",
    "    # plot(d)\n",
    "    value = [d]\n",
    "\n",
    "    if machineId in compete_dataset:\n",
    "        old_data = compete_dataset.get(machineId)\n",
    "\n",
    "        for old in old_data:\n",
    "            value.append(old)\n",
    "\n",
    "    compete_dataset.update({machineId: value})\n",
    "\n",
    "for machineId in compete_dataset.keys():\n",
    "    d = pd.concat(compete_dataset.get(machineId))\n",
    "\n",
    "    d.to_csv(f\"dataset/results/it2/id-{machineId}.csv\", mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT3 ~ Now I use the pearson correlation to see the correlation between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.getSingleDataset.utils import getCleanDataset\n",
    "from scipy import stats\n",
    "from scripts.plots import plot\n",
    "\n",
    "files = [p for p in os.listdir(\"dataset/results/it2\") if p.endswith(\".csv\")]\n",
    "\n",
    "STR = \"\\t\\t{:0.2f}\\n\\t\\tP-Value:{:0.2f}\"\n",
    "\n",
    "def is_correlation_usable(data):\n",
    "    computing_data = data.dropna()\n",
    "\n",
    "    # Correlazione tra Energia e Produzioni\n",
    "    energy_productions_corr, energy_productions_pvalue = stats.pearsonr(\n",
    "        computing_data[\"EnergyConsumption\"], computing_data[\"Productions\"]\n",
    "    )\n",
    "    print(\"\\tCorrelation between Energy Consumption and Productions\")\n",
    "    print(STR.format(energy_productions_corr, energy_productions_pvalue))\n",
    "    if energy_productions_pvalue <= 0.05:\n",
    "        print(\"\\t\\tSignificant\")\n",
    "\n",
    "    # Correlazione tra Energia e Fermate\n",
    "    # energy_stops_corr, energy_stops_pvalue = stats.pearsonr(\n",
    "    #     computing_data[\"EnergyConsumption\"], computing_data[\"Fermate\"]\n",
    "    # )\n",
    "    # print(\"\\tCorrelation between Energy Consumption and Stops\")\n",
    "    # print(STR.format(energy_stops_corr, energy_stops_pvalue))\n",
    "    # if energy_stops_pvalue <= 0.05:\n",
    "    #     print(\"\\t\\tSignificant\")\n",
    "\n",
    "    # Correlazione tra Produzioni e Fermate\n",
    "    # productions_stops_corr, productions_stops_pvalue = stats.pearsonr(\n",
    "    #     computing_data[\"Productions\"], computing_data[\"Stops\"]\n",
    "    # )\n",
    "    # print(\"\\tCorrelation between Productions and Stops\")\n",
    "    # print(STR.format(productions_stops_corr, productions_stops_pvalue))\n",
    "    # if productions_stops_pvalue <= 0.05:\n",
    "    #     print(\"\\t\\tSignificant\")\n",
    "\n",
    "    return True\n",
    "    return (\n",
    "        energy_productions_pvalue <= 0.05\n",
    "        # or energy_stops_pvalue <= 0.05\n",
    "        # or productions_stops_pvalue <= 0.05\n",
    "    )\n",
    "\n",
    "dfs = [(int(file.replace(\".csv\", \"\").split(\"-\")[1]), getCleanDataset(f\"dataset/results/it2/{file}\")) for file in files]\n",
    "\n",
    "usable_datasets = [df for df in dfs if is_correlation_usable(df[1])]\n",
    "\n",
    "print(\"Usable datasets: \", len(usable_datasets))\n",
    "\n",
    "tot = 0\n",
    "for machineId, d in usable_datasets:\n",
    "    d.to_csv(f\"dataset/results/it3/id-{machineId}.csv\", mode=\"w\", index=False)\n",
    "    tot += d.shape[0]\n",
    "    print(\"- Machine\", machineId)\n",
    "    plot(d)\n",
    "\n",
    "# print(\"Total:\", tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to use a linear model to try to predict the lifetime of the machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I try to add material to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTotal rows: 1284491\n",
      "\tTraining rows: 1027592\n",
      "\tPrediction errors 50%\n",
      "percentile: 50%\n",
      "No usable data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scripts.getSingleDataset.getProductions import getProductionWithFixedComma\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "SHIFT_VALUE = 1\n",
    "COLUMNS_TO_SHIFT = {\n",
    "    \"Productions\": \"Previous_Prod\",\n",
    "    \"EnergyConsumption\": \"Previous_EC\",\n",
    "    \"Material_HASH\": \"Previous_Material\",\n",
    "    \"COD_ART_HASH\": \"Previous_COD_ART\",\n",
    "}\n",
    "\n",
    "COLUMNS = [\n",
    "    \"MachineId\",\n",
    "    \"Productions\",\n",
    "    \"EnergyConsumption\",\n",
    "    \"COD_ART_HASH\",\n",
    "    \"Material_HASH\",\n",
    "]\n",
    "COLUMNS += list(COLUMNS_TO_SHIFT.values())\n",
    "\n",
    "# get materials\n",
    "materials = getProductionWithFixedComma(f\"dataset/fermi/FERMATE 2211 ACR.csv\")[\n",
    "    [\"CODART\", \"ACR\"]\n",
    "].drop_duplicates()\n",
    "materials.rename(columns={\"CODART\": \"COD_ART\", \"ACR\": \"Material\"}, inplace=True)\n",
    "\n",
    "# get dataset\n",
    "train_data = pd.DataFrame()\n",
    "for file in os.listdir(\"dataset/results/it3\"):\n",
    "    data = pd.read_csv(f\"dataset/results/it3/{file}\")\n",
    "\n",
    "    data.sort_values(by=\"START_DATE\", inplace=True)\n",
    "\n",
    "    data = pd.merge(data, materials, how=\"left\")\n",
    "    data[\"Material_HASH\"] = data[\"Material\"].apply(hash)\n",
    "\n",
    "    data[\"COD_ART_HASH\"] = data[\"COD_ART\"].apply(hash)\n",
    "\n",
    "    previous_size = data.shape[0]\n",
    "    # add previous data\n",
    "    for c in COLUMNS_TO_SHIFT.keys():\n",
    "        data[COLUMNS_TO_SHIFT[c]] = data[c].shift(-SHIFT_VALUE)\n",
    "    #     print(data[[\"START_DATE\", \"END_DATE\", COLUMNS_TO_SHIFT[c], c]])\n",
    "    assert previous_size == data.shape[0]\n",
    "\n",
    "    data[\"MachineId\"] = int(file.replace(\".csv\", \"\").split(\"-\")[1])\n",
    "\n",
    "    # data = data[data[\"Stop\"] != \"No Stop\"]\n",
    "    data.dropna()\n",
    "\n",
    "    train_data = pd.concat([train_data, data])\n",
    "\n",
    "def calculateModel(dataset: pd.DataFrame, model: HistGradientBoostingClassifier):\n",
    "    print(\"\\tTotal rows:\", dataset.shape[0])\n",
    "    rows = int(dataset.shape[0] * 0.8)\n",
    "    train_d = dataset[:rows]\n",
    "    test_d = dataset[rows:]\n",
    "\n",
    "    if train_d.shape[0] == 0 or test_d.shape[0] == 0:\n",
    "        raise Exception(\n",
    "            \"Not enough data\",\n",
    "            dataset.shape[0],\n",
    "            train_d.shape[0],\n",
    "            test_d.shape[0],\n",
    "        )\n",
    "\n",
    "    # this checks for 0 values\n",
    "    if train_d.shape[0] == 0 or test_d.shape[0] == 0:\n",
    "        raise Exception(f\"\\tNot enough data, train: {train_d.shape[0]}, test: {test_d.shape[0]}\")\n",
    "\n",
    "    print(\"\\tTraining rows:\", train_d.shape[0])\n",
    "\n",
    "    model.fit(train_d[COLUMNS], train_d[\"Stop\"])\n",
    "\n",
    "    # Valutare il modello utilizzando i dati di test\n",
    "    prediction = model.predict(test_d[COLUMNS])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Prediction\": prediction,\n",
    "            \"Real\": test_d[\"Stop\"],\n",
    "            \"Correct\": prediction == test_d[\"Stop\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = df[df[\"Real\"] != \"No Stop\"]\n",
    "\n",
    "    res = df[\"Correct\"].value_counts()\n",
    "\n",
    "    errors = res[False] if False in res else 0\n",
    "\n",
    "    if df.shape[0] == 0:\n",
    "        raise Exception(\"Dataset is not usable\")\n",
    "\n",
    "    percentile = int(errors / df.shape[0] * 100)\n",
    "    print(f\"\\tPrediction errors {percentile}%\")\n",
    "    if percentile == 0:\n",
    "        print(\"---------\")\n",
    "        print(errors, df.shape[0])\n",
    "\n",
    "        print(df)\n",
    "        print(res)\n",
    "\n",
    "    return percentile\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "average = 0\n",
    "usables = 0\n",
    "# for machine_id in train_data[\"MachineId\"].unique():\n",
    "#     print(\"Machine\", machine_id)\n",
    "\n",
    "#     dataset = train_data[train_data[\"MachineId\"] == machine_id]\n",
    "\n",
    "#     try:\n",
    "#         average += calculateModel(dataset, model)\n",
    "#         usables += 1\n",
    "#     except Exception as e:\n",
    "#         print(\"\\t\", e)\n",
    "print(f\"percentile: {calculateModel(train_data, model)}%\")\n",
    "\n",
    "if usables == 0:\n",
    "    print(\"No usable data\")\n",
    "else:\n",
    "    average = average / usables\n",
    "    print(f\"Avg Error: {average:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
