{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts Fermate xls to csv, NOT NEEDED if you have downloaded the dataset from the link in the README\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for l in os.listdir(\"dataset/fermi_excel/Fermate\"):\n",
    "    df = pd.read_excel(f\"dataset/fermi_excel/Fermate/{l}\")\n",
    "    name = l.replace(\".xls\", \"\")\n",
    "\n",
    "    df.to_csv(f\"dataset/fermi/Fermate/{name}.csv\", index=False)\n",
    "\n",
    "df = pd.read_excel(f\"dataset/fermi_excel/FERMATE 2211 ACR.xls\")\n",
    "df.to_csv(f\"dataset/fermi/FERMATE 2211 ACR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I iterate for every machine to get the data from the 3 datasets and merge them into one dataset, removing the ones that are not useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT1 ~ Now we can merge all three datasets and see what we can do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 3 dataset into one and saves in `it1`\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from scripts.getDataset import getEntireDataset, getList\n",
    "\n",
    "DEBUG = True\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.options.mode.copy_on_write = False\n",
    "\n",
    "toskip = [file for file in os.listdir(\"dataset/results/it1\")]\n",
    "\n",
    "complete_dataset = getList()\n",
    "\n",
    "for machineId, year, month in complete_dataset:\n",
    "    filename = f\"id-{machineId}_{year}-{month}.csv\"\n",
    "\n",
    "    if filename in toskip:\n",
    "        print(\"file already exists\", filename)\n",
    "        continue\n",
    "\n",
    "    dataset = getEntireDataset(machineId, year, month, False)\n",
    "\n",
    "    dataset.to_csv(\n",
    "        f\"dataset/results/it1/{filename}\", index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT2 ~ It takes 25 minutes to compute everything but now I want to look at the lifetime of the machines and see if I can find something interesting, merging the data into one file for machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data time series into a single file for each machine, also append material\n",
    "import pandas as pd\n",
    "import os\n",
    "from scripts.getSingleDataset.utils import getCleanDataset, replaceWithUnknown\n",
    "from scripts.getSingleDataset.getProductions import getProductionWithFixedComma\n",
    "\n",
    "# get materials\n",
    "materials = getProductionWithFixedComma(f\"dataset/fermi/FERMATE 2211 ACR.csv\")[\n",
    "    [\"CODART\", \"ACR\"]\n",
    "].drop_duplicates()\n",
    "materials.rename(columns={\"CODART\": \"COD_ART\", \"ACR\": \"Material\"}, inplace=True)\n",
    "\n",
    "complete_dataset = {}\n",
    "\n",
    "for path in os.listdir(\"dataset/results/it1\"):\n",
    "    splitted_filename = [p.split(\"-\") for p in path.replace(\".csv\", \"\").split(\"_\")]\n",
    "\n",
    "    try:\n",
    "        d = getCleanDataset(f\"dataset/results/it1/{path}\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(\"Skipping file is probably empty\", path)\n",
    "        continue\n",
    "\n",
    "    if d.empty:\n",
    "        print(\"Skipping as empty\", path)\n",
    "        continue\n",
    "\n",
    "    machineId = int(splitted_filename[0][1])\n",
    "\n",
    "    d = pd.merge(d, materials, how=\"left\")\n",
    "\n",
    "    v = d[d[\"START_DATE\"] == pd.to_datetime(\"2022-05-30 10:30\")]\n",
    "    \n",
    "    if v.count().sum() > 0:\n",
    "        print(v)\n",
    "\n",
    "    value = [d]\n",
    "\n",
    "    if machineId in complete_dataset:\n",
    "        old_data = complete_dataset.get(machineId)\n",
    "\n",
    "        for old in old_data:\n",
    "            value.append(old)\n",
    "\n",
    "    complete_dataset.update({machineId: value})\n",
    "\n",
    "for machineId in complete_dataset.keys():\n",
    "    df = pd.concat(complete_dataset.get(machineId))\n",
    "    prev_col = df.columns\n",
    "\n",
    "    df = (\n",
    "        df.groupby([\"START_DATE\", \"END_DATE\"])\n",
    "        .agg({\n",
    "            \"Stop\": \"first\",\n",
    "            \"COD_ART\": \"first\",\n",
    "            \"Productions\": \"sum\",\n",
    "            \"EnergyConsumption\": \"sum\",\n",
    "            \"Material\": \"first\",\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    assert (prev_col == df.columns).all(), (prev_col, df.columns)\n",
    "\n",
    "    df = replaceWithUnknown(df)\n",
    "\n",
    "    size = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    assert size == df.shape[0], df.shape[0]\n",
    "\n",
    "    df.to_csv(f\"dataset/results/it2/id-{machineId}.csv\", mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the results to make the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "import os\n",
    "from scripts.plots import plot, correlation_plot\n",
    "from scripts.getSingleDataset.utils import getCleanDataset\n",
    "from scripts.getSingleDataset.utils import takeRange\n",
    "\n",
    "\n",
    "for file in os.listdir(\"dataset/results/it2\"):\n",
    "    df = getCleanDataset(f\"dataset/results/it2/{file}\")\n",
    "    machineId = int(file.replace(\".csv\", \"\").split(\"-\")[1])\n",
    "\n",
    "    df = takeRange(df, 5)\n",
    "    plot(df, machineId)\n",
    "    # correlation_plot(df, machineId)\n",
    "\n",
    "    # if \"x\" == input(\"Press Enter to continue, x to close...\"):\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a Machine learning model to predict when a machine will stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine 304\n",
      "\tTotal rows: 25110\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 310\n",
      "\tTotal rows: 25676\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 305\n",
      "\tTotal rows: 15528\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 515\n",
      "\tTotal rows: 24010\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 313\n",
      "\tTotal rows: 25579\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 307\n",
      "\tTotal rows: 25537\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 110\n",
      "\tTotal rows: 20015\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 306\n",
      "\tTotal rows: 25527\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 302\n",
      "\tTotal rows: 25490\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 303\n",
      "\tTotal rows: 25523\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 301\n",
      "\tTotal rows: 25513\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 315\n",
      "\tTotal rows: 25550\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 314\n",
      "\tTotal rows: 25594\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 614\n",
      "\tTotal rows: 25573\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 611\n",
      "\tTotal rows: 25532\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 610\n",
      "\tTotal rows: 25031\n",
      "\tError: Data is not correctly distributed train have only 'Running'\n",
      "Machine 612\n",
      "\tTotal rows: 25582\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 618\n",
      "\tTotal rows: 25705\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 319\n",
      "\tTotal rows: 25559\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 108\n",
      "\tTotal rows: 19984\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 308\n",
      "\tTotal rows: 18803\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "Machine 309\n",
      "\tTotal rows: 25613\n",
      "\tError: Data is not correctly distributed test have only 'Running'\n",
      "No usable dataset found\n",
      "--- Complete model ---\n",
      "\tTotal rows: 534188\n",
      "\tTraining rows: 320512\n",
      "\tPrediction errors 0%\n",
      "---------\n",
      "0\n",
      "Prediction    0\n",
      "Real          0\n",
      "Correct       0\n",
      "dtype: int64\n",
      "Real\n",
      "Sostituzione utensile         61\n",
      "Manutenzione ordinaria        28\n",
      "Affilatura utensile            9\n",
      "Manutenzione straordinaria     2\n",
      "Name: count, dtype: int64\n",
      "Error: 0%\n"
     ]
    }
   ],
   "source": [
    "# Classic ML analysis\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "SHIFT_VALUE = 2\n",
    "BASE_COLUMNS_TO_SHIFT = {\n",
    "    \"Productions\": \"Productions_prev\",\n",
    "    \"EnergyConsumption\": \"EnergyConsumption_prev\"\n",
    "}\n",
    "\n",
    "shift_columns = [\n",
    "    f\"{c}_{s}\"\n",
    "    for c in BASE_COLUMNS_TO_SHIFT.values()\n",
    "    for s in range(1, SHIFT_VALUE + 1)\n",
    "]\n",
    "\n",
    "COLUMNS = [\n",
    "    \"MachineId\",\n",
    "    \"Productions\",\n",
    "    \"EnergyConsumption\",\n",
    "    \"COD_ART_HASH\",\n",
    "    \"Material_HASH\",\n",
    "] + shift_columns\n",
    "\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "for file in os.listdir(\"dataset/results/it2\"):\n",
    "    data = pd.read_csv(f\"dataset/results/it2/{file}\")\n",
    "\n",
    "    data.sort_values(by=\"START_DATE\", inplace=True)\n",
    "\n",
    "    data[\"Material_HASH\"] = data[\"Material\"].apply(hash)\n",
    "\n",
    "    data[\"COD_ART_HASH\"] = data[\"COD_ART\"].apply(hash)\n",
    "\n",
    "    data[\"MachineId\"] = int(file.replace(\".csv\", \"\").split(\"-\")[1])\n",
    "\n",
    "    data.dropna()\n",
    "\n",
    "    previous_size = data.shape[0]\n",
    "\n",
    "    for s in range(1, SHIFT_VALUE + 1):\n",
    "        for c in BASE_COLUMNS_TO_SHIFT.keys():\n",
    "            data[f\"{BASE_COLUMNS_TO_SHIFT[c]}_{s}\"] = data[c].shift(s)\n",
    "\n",
    "    assert previous_size == data.shape[0]\n",
    "\n",
    "    train_data = pd.concat([train_data, data])\n",
    "\n",
    "\n",
    "# train_data[:10000].to_csv(\"dataset/results/complete.csv\", mode=\"w\", index=False)\n",
    "# print(train_data.columns)\n",
    "\n",
    "def calculateModel(dataset: pd.DataFrame, model: HistGradientBoostingClassifier):\n",
    "    print(\"\\tTotal rows:\", dataset.shape[0])\n",
    "    dataset[\"Stop_index\"] = dataset[\"Stop\"].apply(lambda x: 1 if x == \"Running\" else 0)\n",
    "\n",
    "    rows = int(dataset.shape[0] * 0.6)\n",
    "    train_d = dataset[:rows]\n",
    "    test_d = dataset[rows:]\n",
    "\n",
    "    if train_d[\"Stop\"].value_counts()[\"Running\"] == train_d.shape[0]:\n",
    "        raise Exception(\"Data is not correctly distributed train have only 'Running'\")\n",
    "    if test_d[\"Stop\"].value_counts()[\"Running\"] == test_d.shape[0]:\n",
    "        raise Exception(\"Data is not correctly distributed test have only 'Running'\")\n",
    "\n",
    "    if train_d.shape[0] == 0 or test_d.shape[0] == 0:\n",
    "        raise Exception(\n",
    "            f\"\\tNot enough data, dataset: {dataset.shape[0]},  train: {train_d.shape[0]}, test: {test_d.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\tTraining rows:\", train_d.shape[0])\n",
    "\n",
    "    model.fit(train_d[COLUMNS], train_d[\"Stop_index\"])\n",
    "\n",
    "    # Valutare il modello utilizzando i dati di test\n",
    "    prediction = model.predict(test_d[COLUMNS])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Prediction\": prediction,\n",
    "            \"Real\": test_d[\"Stop\"],\n",
    "            \"Correct\": prediction == test_d[\"Stop_index\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    assert df.shape[0] == test_d.shape[0]\n",
    "\n",
    "    if df.shape[0] == 0:\n",
    "        raise Exception(\"Dataset is not usable\")\n",
    "\n",
    "    # this is only useful to remove the \"Running\" values from the calculations of the percentile\n",
    "    # print(df.drop_duplicates())\n",
    "    # print(df[df[\"Real\"] != \"Running\"])\n",
    "    df = df[df[\"Real\"] != \"Running\"]\n",
    "\n",
    "    if df.shape[0] == 0:\n",
    "        raise Exception(\"No data to calculate\")\n",
    "\n",
    "    res = df[\"Real\"].value_counts()\n",
    "\n",
    "    errors = res[False] if False in res else 0\n",
    "    percentile = int(errors / df.shape[0] * 100)\n",
    "\n",
    "    print(f\"\\tPrediction errors {percentile}%\")\n",
    "    if percentile == 0:\n",
    "        print(\"---------\")\n",
    "        print(errors)#, df.shape[0])\n",
    "\n",
    "        print(df[df[\"Correct\"] == True].count())\n",
    "        print(res)\n",
    "\n",
    "    return percentile\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "average = 0\n",
    "usables = 0\n",
    "for machine_id in train_data[\"MachineId\"].unique():\n",
    "    print(\"Machine\", machine_id)\n",
    "\n",
    "    dataset = train_data[train_data[\"MachineId\"] == machine_id]\n",
    "\n",
    "    try:\n",
    "        dataset = dataset.dropna()\n",
    "        average += calculateModel(dataset, model)\n",
    "        usables += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\tError:\", e)\n",
    "if usables == 0:\n",
    "    print(\"No usable dataset found\")\n",
    "else:\n",
    "    average = average / usables\n",
    "    print(f\"Avg Error: {average:.2f}%\")\n",
    "\n",
    "print(\"--- Complete model ---\")\n",
    "print(f\"Error: {calculateModel(train_data, model)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use PySpark to implement MlLib and train a model to predict when a machine will stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 15:11:36 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.25.147 instead (on interface en0)\n",
      "24/03/20 15:11:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/20 15:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTotal rows: 25112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrediction errors 0.0000%\n",
      "Machine 310\n",
      "\tTotal rows: 25678\n",
      "\tPrediction errors 0.0000%\n",
      "Machine 305\n",
      "\tTotal rows: 15530\n",
      "\tPrediction errors 0.0964%\n",
      "Machine 515\n",
      "\tTotal rows: 25542\n",
      "\tPrediction errors 0.0394%\n",
      "Machine 313\n",
      "\tTotal rows: 25581\n",
      "\tPrediction errors 0.0195%\n",
      "Machine 307\n",
      "\tTotal rows: 25539\n",
      "\tPrediction errors 0.0391%\n",
      "Machine 110\n",
      "\tTotal rows: 20017\n",
      "\tPrediction errors 0.0740%\n",
      "Machine 306\n",
      "\tTotal rows: 25529\n",
      "\tPrediction errors 0.0398%\n",
      "Machine 302\n",
      "\tTotal rows: 25492\n",
      "\tPrediction errors 0.0000%\n",
      "Machine 303\n",
      "\tTotal rows: 25525\n",
      "\tPrediction errors 0.0776%\n",
      "Machine 301\n",
      "\tTotal rows: 25515\n",
      "\tPrediction errors 0.0377%\n",
      "Machine 315\n",
      "\tTotal rows: 25552\n",
      "\tPrediction errors 0.0191%\n",
      "Machine 314\n",
      "\tTotal rows: 25596\n",
      "\tPrediction errors 0.0196%\n",
      "Machine 614\n",
      "\tTotal rows: 25575\n",
      "\tPrediction errors 0.1183%\n",
      "Machine 611\n",
      "\tTotal rows: 25534\n",
      "\tPrediction errors 0.0197%\n",
      "Machine 610\n",
      "\tTotal rows: 25613\n",
      "\tPrediction errors 0.0197%\n",
      "Machine 612\n",
      "\tTotal rows: 25584\n",
      "\tPrediction errors 0.0402%\n",
      "Machine 618\n",
      "\tTotal rows: 25707\n",
      "\tPrediction errors 0.0804%\n",
      "Machine 319\n",
      "\tTotal rows: 25561\n",
      "\tPrediction errors 0.0394%\n",
      "Machine 108\n",
      "\tTotal rows: 19986\n",
      "\tPrediction errors 0.0503%\n",
      "Machine 308\n",
      "\tTotal rows: 18805\n",
      "\tPrediction errors 0.0270%\n",
      "Machine 309\n",
      "\tTotal rows: 25615\n",
      "\tPrediction errors 0.0383%\n",
      "Avg Error: 0.04%\n"
     ]
    }
   ],
   "source": [
    "# Big Data ML analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, isnan\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "SHIFT_VALUE = 4\n",
    "BASE_COLUMNS_TO_SHIFT = {\n",
    "    \"Productions\": \"Productions_prev\",\n",
    "    \"EnergyConsumption\": \"EnergyConsumption_prev\",\n",
    "    \"Material_HASH\": \"Material_Previous\",\n",
    "    \"COD_ART_HASH\": \"COD_ART_Previous\",\n",
    "}\n",
    "\n",
    "shift_columns = [\n",
    "    f\"{c}_{s}\"\n",
    "    for c in BASE_COLUMNS_TO_SHIFT.values()\n",
    "    for s in range(1, SHIFT_VALUE + 1)\n",
    "]\n",
    "\n",
    "COLUMNS = [\n",
    "    \"MachineId\",\n",
    "    \"Productions\",\n",
    "    \"EnergyConsumption\",\n",
    "    \"COD_ART_HASH\",\n",
    "    \"Material_HASH\",\n",
    "] + shift_columns\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "for file in os.listdir(\"dataset/results/it2\"):\n",
    "    data = pd.read_csv(f\"dataset/results/it2/{file}\")\n",
    "\n",
    "    data.sort_values(by=\"START_DATE\", inplace=True)\n",
    "\n",
    "    data[\"Material_HASH\"] = data[\"Material\"].apply(hash)\n",
    "\n",
    "    data[\"COD_ART_HASH\"] = data[\"COD_ART\"].apply(hash)\n",
    "\n",
    "    data[\"Stop_index\"] = data[\"Stop\"].apply(lambda x: 1 if x == \"Running\" else 0)\n",
    "\n",
    "    data[\"MachineId\"] = int(file.replace(\".csv\", \"\").split(\"-\")[1])\n",
    "\n",
    "    data.dropna()\n",
    "\n",
    "    previous_size = data.shape[0]\n",
    "\n",
    "    for s in range(1, SHIFT_VALUE + 1):\n",
    "        for c in BASE_COLUMNS_TO_SHIFT.keys():\n",
    "            data[f\"{BASE_COLUMNS_TO_SHIFT[c]}_{s}\"] = data[c].shift(s)\n",
    "\n",
    "    assert previous_size == data.shape[0]\n",
    "\n",
    "    train_data = pd.concat([train_data, data])\n",
    "\n",
    "train_data.to_csv(\"dataset/results/complete.csv\", mode=\"w\", index=False)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=COLUMNS,outputCol=\"features\", handleInvalid=\"keep\")\n",
    "labelIndexer = StringIndexer(\n",
    "    inputCol=\"Stop_index\", outputCol=\"indexedLabel\", handleInvalid=\"keep\"\n",
    ")\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4, handleInvalid=\"keep\")\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\",featuresCol=\"indexedFeatures\",maxIter=10)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# MlLib\n",
    "def calculateModel(dataset: pd.DataFrame):\n",
    "    dataset = spark.createDataFrame(dataset)\n",
    "\n",
    "    print(\"\\tTotal rows:\", dataset.count())\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        dataset = dataset.withColumn(\n",
    "            col, when(isnan(dataset[col]), 0).otherwise(dataset[col])\n",
    "        )\n",
    "\n",
    "    dataset = assembler.transform(dataset)\n",
    "    dataset = dataset.na.fill(0)\n",
    "\n",
    "    train_d, test_d = dataset.randomSplit([0.8, 0.2])\n",
    "\n",
    "    if train_d.count() == 0 or test_d.count() == 0:\n",
    "        raise Exception(\n",
    "            \"Not enough data\", dataset.count(), train_d.count(), test_d.count()\n",
    "        )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        stages=[labelIndexer.fit(train_d), featureIndexer.fit(train_d), gbt],\n",
    "    )\n",
    "\n",
    "    model = pipeline.fit(train_d)\n",
    "\n",
    "    prediction = model.transform(test_d)\n",
    "\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "\n",
    "    percentile = (1-accuracy) * 100\n",
    "    print(f\"\\tPrediction errors {percentile:.4f}%\")\n",
    "\n",
    "    return percentile\n",
    "\n",
    "average = 0\n",
    "usables = 0\n",
    "for machine_id in train_data[\"MachineId\"].unique():\n",
    "    print(\"Machine\", machine_id)\n",
    "\n",
    "    dataset = train_data[train_data[\"MachineId\"] == machine_id]\n",
    "\n",
    "    try:\n",
    "        average += calculateModel(dataset)\n",
    "        usables += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\t\", e)\n",
    "\n",
    "if usables == 0:\n",
    "    print(\"No usable data\")\n",
    "else:\n",
    "    average = average / usables\n",
    "    print(f\"Avg Error: {average:.2f}%\")\n",
    "\n",
    "# print(f\"Complete model\\n\\tError: {calculateModel(train_data)}%\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
